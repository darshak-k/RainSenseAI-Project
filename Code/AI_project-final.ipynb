{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KDZoEnQLGRog"
      },
      "outputs": [],
      "source": [
        "#importing important libraries\n",
        "import os\n",
        "import sys\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from tabulate import tabulate\n",
        "from scipy.stats import skew\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.array([1, 2, 3])"
      ],
      "metadata": {
        "id": "YJR_F63mGjlH",
        "outputId": "36d9fbfb-ac09-40c0-bfa5-e62ef25e9742",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 2, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eUJVsXBdGRoi"
      },
      "outputs": [],
      "source": [
        "#HELPER FUNCTIONS FOR PREPROCESSING DATA\n",
        "\n",
        "def replaceNullValAppropriately(dataset):\n",
        "    skewed_cols = []\n",
        "    non_skewed_cols = []\n",
        "    for col in dataset.columns:\n",
        "        if col=='RainToday':\n",
        "            continue\n",
        "        if abs(skew(dataset[col].dropna())) > 1.0:\n",
        "            skewed_cols.append(col)\n",
        "        else:\n",
        "            non_skewed_cols.append(col)\n",
        "\n",
        "    if 'RainToday' in skewed_cols:\n",
        "        skewed_cols.remove('RainToday')\n",
        "    if 'RainToday' in non_skewed_cols:\n",
        "        non_skewed_cols.remove('RainToday')\n",
        "    # dataset['RainToday'] = [None if math.isnan(x) else x for x in dataset['RainToday']]\n",
        "    dataset[skewed_cols] = dataset[skewed_cols].fillna(dataset[skewed_cols].median())\n",
        "    dataset[non_skewed_cols] = dataset[non_skewed_cols].fillna(dataset[non_skewed_cols].mean())\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VfgaWRWlGRoj"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from tabulate import tabulate\n",
        "\n",
        "def return_int_columns(dataset):\n",
        "    selected_columns = []\n",
        "    for column in dataset.columns:\n",
        "        if not dataset[column].apply(lambda x: isinstance(x, str)).any():\n",
        "            selected_columns.append(column)\n",
        "    return dataset[selected_columns]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-ARecOEGRoj"
      },
      "outputs": [],
      "source": [
        "def check_missing_value(dataset):\n",
        "    # Show the summary statistics of the dataset\n",
        "    summary_stats = dataset.describe().transpose()\n",
        "    print(\"Summary Statistics:\")\n",
        "    print(tabulate(summary_stats, headers=\"keys\", tablefmt=\"psql\"))\n",
        "    print()\n",
        "\n",
        "    # Show the number of missing values for each column in the dataset\n",
        "    missing_values = dataset.isnull().sum().to_frame(name=\"Missing Values\")\n",
        "    print(\"Missing Values:\")\n",
        "    print(tabulate(missing_values, headers=\"keys\", tablefmt=\"psql\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zxpSaT0YGRoj"
      },
      "outputs": [],
      "source": [
        "def calculate_spearman_corr(dataset, target_variable):\n",
        "    # This function calculates the Spearman correlation between each variable and the target variable.\n",
        "    df_corr = pd.DataFrame({'features': dataset.columns})\n",
        "    df_corr['spearman'] = [dataset[c].corr(dataset[target_variable], method='spearman') for c in dataset.columns]\n",
        "    df_corr = df_corr.sort_values('spearman')\n",
        "    return df_corr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88oT5zhWGRoj"
      },
      "outputs": [],
      "source": [
        "def plot_spearman_corr(df_corr):\n",
        "    # This function plots the Spearman correlation for each variable.\n",
        "    plt.figure(figsize=(6, 0.25 * len(df_corr)))\n",
        "    sns.barplot(data=df_corr, y='features', x='spearman', orient='h')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BI3eWBy9GRok"
      },
      "outputs": [],
      "source": [
        "def spearman_corr(dataset):\n",
        "    # This function analyzes the Spearman correlation between each variable and three different target variables.\n",
        "    target_variables = ['Rainfall']\n",
        "    for target_variable in target_variables:\n",
        "        df_corr = calculate_spearman_corr(dataset, target_variable)\n",
        "        plot_spearman_corr(df_corr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ej_fvbgGRok"
      },
      "outputs": [],
      "source": [
        "def pearson_corr_heatmap(dataset):\n",
        "    \"\"\"\n",
        "    This function is used to analyze the pearson correlation between each variable including the target variable.\n",
        "    Generally to reduce confounding, only variables uncorrelated with each other should be added to regression models.\n",
        "    \"\"\"\n",
        "\n",
        "    correlation_matrix = dataset.corr()\n",
        "\n",
        "    # Plot the correlation matrix as a heatmap\n",
        "    plt.figure(figsize=(20, 20))\n",
        "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
        "    plt.title('Correlation Matrix')\n",
        "    plt.savefig('pearson_corr_heatmap.png', bbox_inches='tight')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dy_3vfhNGRok"
      },
      "outputs": [],
      "source": [
        "def convert_wind_direction(dataset):\n",
        "    directions = {\n",
        "        'N': 0,\n",
        "        'NNE': 22.5,\n",
        "        'NE': 45,\n",
        "        'ENE': 67.5,\n",
        "        'E': 90,\n",
        "        'ESE': 112.5,\n",
        "        'SE': 135,\n",
        "        'SSE': 157.5,\n",
        "        'S': 180,\n",
        "        'SSW': 202.5,\n",
        "        'SW': 225,\n",
        "        'WSW': 247.5,\n",
        "        'W': 270,\n",
        "        'WNW': 292.5,\n",
        "        'NW': 315,\n",
        "        'NNW': 337.5\n",
        "    }\n",
        "\n",
        "    # Apply the conversion using map()\n",
        "    dataset['WindGustDir_num'] = dataset['WindGustDir'].map(directions)\n",
        "    dataset['WindDir9am_num'] = dataset['WindDir9am'].map(directions)\n",
        "    dataset['WindDir3pm_num'] = dataset['WindDir3pm'].map(directions)\n",
        "    dataset['RainTodayNum'] = dataset['RainToday'].str.lower().map({'yes': 1, 'no': 0})\n",
        "    dataset['RainTomorrowNum'] = dataset['RainTomorrow'].str.lower().map({'yes': 1, 'no': 0})\n",
        "    dataset['location_encoded'], _ = pd.factorize(dataset['Location'])\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VLPVZvTVGRol"
      },
      "outputs": [],
      "source": [
        "# STATISTICAL DATA VIEW\n",
        "\n",
        "data_visualize = pd.read_csv('weatherAUS.csv')\n",
        "check_missing_value(data_visualize)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-HrZoEFGRol"
      },
      "source": [
        "### MANIPULATE WIND DIRECTION TO INT FOR THE FETURE ENGINEERING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SVU4zhcBGRom"
      },
      "outputs": [],
      "source": [
        "actual_data = data_visualize\n",
        "data_visualize = convert_wind_direction(data_visualize)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZHOtxTHtGRom"
      },
      "outputs": [],
      "source": [
        "data_visualize = return_int_columns(data_visualize)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7GvwbLcOGRom"
      },
      "outputs": [],
      "source": [
        "pearson_corr_heatmap(data_visualize)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNGd1FFeGRom"
      },
      "source": [
        "**Spearman corelation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZCV6psmdGRon"
      },
      "outputs": [],
      "source": [
        "lis = actual_data.columns\n",
        "lis = lis.delete(lis.get_loc('RainTodayNum'))\n",
        "spearman_corr(actual_data[lis])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XZNLqdxGRon"
      },
      "source": [
        "**Data loading, preprocessing, splitting and preparation for modeling.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TDM6_IChGRon"
      },
      "outputs": [],
      "source": [
        "# Define a list of column names to select from the dataset\n",
        "lis = ['Cloud3pm', 'Cloud9am','Humidity9am', 'Humidity3pm','Sunshine','Evaporation','Rainfall','RainToday']\n",
        "data = actual_data[lis]\n",
        "# Split the preprocessed data into training and testing sets\n",
        "train_df, test_df = train_test_split(data, test_size=0.3, random_state=42)\n",
        "# # MISSING DATA HANDLING AND NULL VALUE REPLACEMENT\n",
        "handled_train_data = replaceNullValAppropriately(train_df)\n",
        "handled_test_data = replaceNullValAppropriately(test_df)\n",
        "handled_train_data['RainToday'] = handled_train_data['RainToday'].ffill()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BrNz99NuGRon"
      },
      "outputs": [],
      "source": [
        "# CHECKING THE LENGTH OF SPLIT DATA\n",
        "print(len(train_df))\n",
        "print(len(test_df))\n",
        "print(train_df['RainToday'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUAGtytUGRon"
      },
      "source": [
        "### ***Decision Tree Classification model***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aA59CH1OGRoo"
      },
      "outputs": [],
      "source": [
        "## imports for the decision tree\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import GridSearchCV\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b2sV0OC7GRoo"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "dt_classifier_with_parameters = DecisionTreeClassifier()\n",
        "dt_classifier_with_parameters.fit(handled_train_data.iloc[:, :-1], handled_train_data['RainToday'])\n",
        "# Get the column names of the training data\n",
        "y_pred = dt_classifier_with_parameters.predict(handled_test_data.iloc[:, :-1])\n",
        "# Creating predicted column to compare\n",
        "handled_test_data['RainToday_pred'] = y_pred\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-tfHdkX2GRoo"
      },
      "outputs": [],
      "source": [
        "# Calculate accuracy\n",
        "accuracy = (handled_test_data['RainToday'] == handled_test_data['RainToday_pred']).mean()\n",
        "print(\"Accuracy:\", accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NErNIsv1GRoo"
      },
      "outputs": [],
      "source": [
        "# Define a list of column names to select from the dataset\n",
        "lis = ['Cloud3pm', 'Cloud9am','Humidity9am', 'Humidity3pm','Sunshine','Evaporation','Rainfall','RainToday']\n",
        "data = actual_data[lis]\n",
        "# Split the preprocessed data into training and testing sets\n",
        "train_df, test_df = train_test_split(data, test_size=0.3, random_state=42)\n",
        "# # MISSING DATA HANDLING AND NULL VALUE REPLACEMENT\n",
        "handled_train_data = replaceNullValAppropriately(train_df)\n",
        "handled_test_data = replaceNullValAppropriately(test_df)\n",
        "handled_train_data['RainToday'] = handled_train_data['RainToday'].ffill()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IRbq2DUwGRoo"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Separate features and target in the training data\n",
        "X = handled_train_data.iloc[:, :-1]\n",
        "y = handled_train_data['RainToday']\n",
        "\n",
        "# Create an instance of the decision tree classifier\n",
        "dt_classifier = DecisionTreeClassifier()\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid = {\n",
        "    'criterion': ['gini', 'entropy'],\n",
        "    'max_depth': [3, 5, 10],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 3, 5]\n",
        "}\n",
        "\n",
        "# Create an instance of GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=dt_classifier, param_grid=param_grid, scoring='accuracy', cv=5)\n",
        "\n",
        "# Perform grid search with cross-validation\n",
        "grid_search.fit(X, y)\n",
        "\n",
        "# Get the best hyperparameters\n",
        "best_params = grid_search.best_params_\n",
        "print(\"Best Hyperparameters:\", best_params)\n",
        "\n",
        "# Create an instance of the decision tree classifier with the best hyperparameters\n",
        "dt_classifier_best = DecisionTreeClassifier(**best_params)\n",
        "\n",
        "# Perform k-fold cross-validation\n",
        "k = 5  # Define the number of folds\n",
        "scores = cross_val_score(dt_classifier_best, X, y, cv=k, scoring='accuracy')\n",
        "\n",
        "# Calculate the mean accuracy score\n",
        "mean_accuracy = scores.mean()\n",
        "print(\"Mean Accuracy Score:\", mean_accuracy)\n",
        "\n",
        "# Fit the classifier on the entire training data\n",
        "dt_classifier_best.fit(X, y)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = dt_classifier_best.predict(handled_test_data.iloc[:, :-1])\n",
        "\n",
        "# Add predicted column to the test data\n",
        "handled_test_data['RainToday_pred'] = y_pred\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4xTSlzoxGRop"
      },
      "outputs": [],
      "source": [
        "# Define a list of column names to select from the dataset\n",
        "lis = ['Cloud3pm', 'Cloud9am','Humidity9am', 'Humidity3pm','Sunshine','Evaporation','Rainfall','RainToday']\n",
        "data = actual_data[lis]\n",
        "# Split the preprocessed data into training and testing sets\n",
        "train_df, test_df = train_test_split(data, test_size=0.3, random_state=42)\n",
        "# # MISSING DATA HANDLING AND NULL VALUE REPLACEMENT\n",
        "handled_train_data = replaceNullValAppropriately(train_df)\n",
        "handled_test_data = replaceNullValAppropriately(test_df)\n",
        "handled_train_data['RainToday'] = handled_train_data['RainToday'].ffill()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PrsMn3DGRop"
      },
      "source": [
        "### ***Semi-supervised Learning with Decision Trees***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjnpW6qLGRop"
      },
      "outputs": [],
      "source": [
        "#semi-supervised\n",
        "import pandas as pd\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def split_data(dataset):\n",
        "    train_df, test_df = train_test_split(dataset, test_size=0.15, random_state=42)\n",
        "    handled_train_data = replaceNullValAppropriately(train_df)\n",
        "    handled_test_data = replaceNullValAppropriately(test_df)\n",
        "    handled_train_data['RainToday'] = handled_train_data['RainToday'].ffill()\n",
        "    return handled_train_data\n",
        "\n",
        "\n",
        "desired_accuracy = 0.974\n",
        "current_accuracy_total = 0\n",
        "iteration = 0\n",
        "\n",
        "lis = ['Cloud3pm', 'Cloud9am', 'Humidity9am', 'Humidity3pm', 'Sunshine', 'Evaporation', 'Rainfall', 'RainToday']\n",
        "data = actual_data[lis]\n",
        "train_df, test_df = train_test_split(data, test_size=0.15, random_state=42)\n",
        "handled_train_data = replaceNullValAppropriately(train_df)\n",
        "handled_test_data = replaceNullValAppropriately(test_df)\n",
        "handled_train_data['RainToday'] = handled_train_data['RainToday'].ffill()\n",
        "trained_data = handled_train_data\n",
        "# trained_data = split_data(data)\n",
        "\n",
        "while current_accuracy_total < desired_accuracy:\n",
        "\n",
        "    #replace = False, is to get 20% unique labelled data everytime from trained data set\n",
        "    labeled_idx = np.random.choice(len(trained_data), size=int(len(trained_data) * 0.2),replace=False)\n",
        "    labeled_data = trained_data.iloc[labeled_idx]\n",
        "\n",
        "    # Get unlabeled data\n",
        "    unlabeled_idx = np.setdiff1d(np.arange(len(trained_data)), labeled_idx)\n",
        "    unlabeled_data = trained_data.iloc[unlabeled_idx]\n",
        "\n",
        "    # Train a supervised learning model on the labeled data\n",
        "    dt_classifier = DecisionTreeClassifier()\n",
        "\n",
        "    # Fit the model on the labeled data\n",
        "    dt_classifier.fit(labeled_data.iloc[:, :-1], labeled_data['RainToday'])\n",
        "\n",
        "    # Use the trained model to predict the labels for the unlabeled data\n",
        "    pseudo_labels = dt_classifier.predict(unlabeled_data.iloc[:, :-1])\n",
        "\n",
        "    # Check the probability estimates of the model on the unlabeled data\n",
        "    probas = dt_classifier.predict_proba(unlabeled_data.iloc[:, :-1])\n",
        "    high_confidence_idx = np.where(np.max(probas, axis=1) >= 0.85)[0]\n",
        "    high_confidence_labels = pseudo_labels[high_confidence_idx]\n",
        "    high_confidence_data = unlabeled_data.iloc[high_confidence_idx, :-1]\n",
        "\n",
        "    # Mix the labeled data with the high-confidence pseudo-labeled data and retrain the model\n",
        "    mixed_labeled_data = pd.concat([labeled_data.iloc[:, :-1], high_confidence_data])\n",
        "    mixed_labels = np.concatenate([labeled_data['RainToday'], high_confidence_labels])\n",
        "    dt_classifier.fit(mixed_labeled_data, mixed_labels)\n",
        "    # Make predictions on the testing data\n",
        "    y_pred = dt_classifier.predict(handled_test_data.iloc[:, :-1])\n",
        "    current_accuracy = (handled_test_data['RainToday'] == y_pred).mean()\n",
        "    iteration += 1\n",
        "    print(f\"Iteration {iteration}: Accuracy = {current_accuracy}\")\n",
        "#     print(type(current_accuracy),type(desired_accuracy))\n",
        "    current_accuracy_total = current_accuracy\n",
        "\n",
        "    #now trained data will be labelled and psuedo labeled data\n",
        "    mixed_labels_df = pd.DataFrame(mixed_labels, columns=['RainToday'])\n",
        "    mixed_labeled_data['RainToday'] = mixed_labels_df['RainToday'].values\n",
        "    trained_data = mixed_labeled_data.dropna()\n",
        "#     print(trained_data)\n",
        "    # spliting the data set to avoid overfitting\n",
        "    trained_data = split_data(trained_data)\n",
        "\n",
        "\n",
        "print(f\"Desired accuracy {desired_accuracy} achieved!\")\n",
        "# Creating a predicted column to compare\n",
        "handled_test_data['RainToday_pred'] = y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NPztk4hwGRop"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "test_evaporation = handled_test_data['Evaporation']\n",
        "test_rainfall = handled_test_data['Rainfall']\n",
        "\n",
        "test_combined = np.column_stack((test_evaporation, test_rainfall))\n",
        "\n",
        "# Apply t-SNE to reduce dimensionality to 2D\n",
        "tsne = TSNE(n_components=2, random_state=42,n_iter=500)\n",
        "X_tsne = tsne.fit_transform(test_combined)\n",
        "print(\"handled test data:\", X_tsne)\n",
        "\n",
        "# Convert string labels to numeric values\n",
        "label_encoder = LabelEncoder()\n",
        "y_pred_numeric = label_encoder.fit_transform(y_pred)\n",
        "\n",
        "print(\"Predicted data\", y_pred_numeric)\n",
        "\n",
        "# Create a custom colormap\n",
        "custom_cmap = ListedColormap(['red', 'blue'])\n",
        "\n",
        "# Plot the t-SNE visualization\n",
        "plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y_pred_numeric, cmap=custom_cmap)\n",
        "plt.colorbar()\n",
        "plt.title('t-SNE Visualization of Weather Features with Rain Labels')\n",
        "plt.xlabel('Evaporation')\n",
        "plt.ylabel('Rainfall')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICsOqjqdGRop"
      },
      "source": [
        "### ***Neural networks (RNN)***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lRMcmdCNGRoq"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from torch import nn, optim\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import preprocessing\n",
        "import torch.nn.functional as func\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.impute import SimpleImputer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LLtooguGGRoq"
      },
      "outputs": [],
      "source": [
        "#load the dataset\n",
        "data_visualize = pd.read_csv('weatherAUS.csv')\n",
        "check_missing_value(data_visualize)\n",
        "data_visualize['RainToday'].value_counts()/len(data_visualize)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GKqeSrjqGRoq"
      },
      "outputs": [],
      "source": [
        "#splitting data in train and test sets\n",
        "lis = ['Cloud3pm', 'Cloud9am', 'Humidity9am', 'Humidity3pm', 'Sunshine', 'Evaporation', 'Rainfall', 'RainToday']\n",
        "data = actual_data[lis]\n",
        "train_df, test_df = train_test_split(data, test_size=0.3, random_state=42)\n",
        "\n",
        "# Handle NaN values in categorical columns using most frequent strategy\n",
        "categorical_columns = ['RainToday']  # Specify the categorical column(s) here\n",
        "categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
        "train_df['RainToday'] = categorical_imputer.fit_transform(train_df['RainToday'].values.reshape(-1, 1)).flatten()\n",
        "test_df['RainToday'] = categorical_imputer.transform(test_df['RainToday'].values.reshape(-1, 1)).flatten()\n",
        "\n",
        "handled_train_data = replaceNullValAppropriately(train_df)\n",
        "handled_test_data = replaceNullValAppropriately(test_df)\n",
        "# handled_train_data['RainToday'] = handled_train_data['RainToday'].ffill()\n",
        "\n",
        "#label encoding\n",
        "label_encoder = LabelEncoder()\n",
        "handled_train_data['RainToday'] = label_encoder.fit_transform(handled_train_data['RainToday'])\n",
        "handled_test_data['RainToday'] = label_encoder.transform(handled_test_data['RainToday'])\n",
        "\n",
        "#converting it to Tensors\n",
        "handled_train_data = torch.from_numpy(handled_train_data.to_numpy())\n",
        "handled_test_data = torch.from_numpy(handled_test_data.to_numpy())\n",
        "\n",
        "print(handled_train_data.shape)\n",
        "print(handled_test_data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o4oUu8m4GRoq"
      },
      "outputs": [],
      "source": [
        "# creating the model and taking 8 features into consideration\n",
        "class Model(nn.Module):\n",
        "  def __init__(self, n_features):\n",
        "    super(Model, self).__init__()\n",
        "    self.fc1 = nn.Linear(n_features, 8)\n",
        "    #having three hidden layers between input and output\n",
        "    self.fc2 = nn.Linear(8, 5)     #first hidden layers contains 5 neurons\n",
        "    self.fc3 = nn.Linear(5, 3)     #second hidden layers contains 3 neurons\n",
        "    self.fc4 = nn.Linear(3, 1)     #third hidden layers contains 1 neuron\n",
        "  def forward(self, x):\n",
        "    x = func.relu(self.fc1(x))\n",
        "    x = func.relu(self.fc2(x))\n",
        "    x = func.relu(self.fc3(x))\n",
        "    return torch.sigmoid(self.fc4(x)) #sigmoid function on output layer\n",
        "\n",
        "model = Model(handled_train_data.shape[1]) #passing number of features in train data set\n",
        "\n",
        "#train the model\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "optimiser = optim.Adam(model.parameters(), lr = 0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7eaJNUqdGRoq"
      },
      "outputs": [],
      "source": [
        "def calculate_accuracy(y_true, y_pred):\n",
        "  predicted = y_pred.ge(.5).view(-1)\n",
        "  return (y_true == predicted).sum().float() / len(y_true)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yoVgEYcUGRoq"
      },
      "outputs": [],
      "source": [
        "def round_tensor(t, decimal_places = 3):\n",
        "  return round(t.item(), decimal_places)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WnaG90OzGRoq"
      },
      "outputs": [],
      "source": [
        "# run the model\n",
        "for epoch in range(100):\n",
        "    y_pred = model(handled_train_data.float())\n",
        "    y_pred = torch.squeeze(y_pred)\n",
        "    train_loss = criterion(y_pred, handled_train_data[:, -1].float())\n",
        "    if epoch % 20 == 0:\n",
        "      train_acc = calculate_accuracy( handled_train_data[:, -1].float(), y_pred)\n",
        "      y_test_pred = model(handled_test_data.float())\n",
        "      y_test_pred = torch.squeeze(y_test_pred)\n",
        "      test_loss = criterion(y_test_pred, handled_test_data[:, -1].float())\n",
        "      test_acc = calculate_accuracy(handled_test_data[:, -1].float(), y_test_pred)\n",
        "      print (str('epoch ') + str(epoch) + str(' Train set: loss: ') + str(round_tensor(train_loss)) + str(', accuracy: ') + str(round_tensor(train_acc)) + str(' Test  set: loss: ') + str(round_tensor(test_loss)) + str(', accuracy: ') + str(round_tensor(test_acc)))\n",
        "    optimiser.zero_grad()\n",
        "    train_loss.backward()\n",
        "    optimiser.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cH73HxqpGRor"
      },
      "outputs": [],
      "source": [
        "#to check later\n",
        "classes = ['No rain', 'Raining']\n",
        "\n",
        "y_pred = model(handled_test_data.float())\n",
        "y_pred = y_pred.ge(.5).view(-1)\n",
        "\n",
        "print(classification_report(handled_test_data[:, -1], y_pred, target_names=classes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TAtgoeooGRor"
      },
      "outputs": [],
      "source": [
        "conf_mat = confusion_matrix(handled_test_data[:, -1], y_pred)\n",
        "df_conf_mat = pd.DataFrame(conf_mat, index = classes, columns = classes)\n",
        "heat_map = sns.heatmap(df_conf_mat, annot = True, fmt = 'd')\n",
        "heat_map.yaxis.set_ticklabels(heat_map.yaxis.get_ticklabels(), ha = 'right')\n",
        "heat_map.xaxis.set_ticklabels(heat_map.xaxis.get_ticklabels(), ha = 'right')\n",
        "plt.ylabel('Actual label')\n",
        "plt.xlabel('Predicted label')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4j4D6VyBGRor"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "\n",
        "print(\"handled test data:\", handled_test_data.cpu().numpy().shape)\n",
        "\n",
        "test_evaporation = handled_test_data[:,5] #evaporation\n",
        "test_rainfall = handled_test_data[:,6] #rainfall\n",
        "\n",
        "test_combined = np.column_stack((test_evaporation, test_rainfall))\n",
        "\n",
        "# Apply t-SNE to reduce dimensionality to 2D\n",
        "tsne = TSNE(n_components=2, random_state=42,n_iter=500)\n",
        "X_tsne = tsne.fit_transform(test_combined)\n",
        "print(\"handled test data:\", X_tsne)\n",
        "\n",
        "# y_pred = y_pred.cpu().numpy()\n",
        "\n",
        "# # Convert string labels to numeric values\n",
        "# label_encoder = LabelEncoder()\n",
        "# y_pred_numeric = label_encoder.fit_transform(y_pred)\n",
        "\n",
        "# y_pred = y_pred.cpu().numpy()\n",
        "print(\"Predicted data\", y_pred)\n",
        "\n",
        "# Create a custom colormap\n",
        "custom_cmap = ListedColormap(['red', 'blue'])\n",
        "\n",
        "# Plot the t-SNE visualization\n",
        "plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y_pred, cmap=custom_cmap)\n",
        "plt.colorbar()\n",
        "plt.title('t-SNE Visualization of Weather Features with Rain Labels')\n",
        "plt.xlabel('Evaporation')\n",
        "plt.ylabel('Rainfall')\n",
        "plt.show()\n",
        "\n",
        "# # Apply t-SNE to reduce dimensionality to 2D\n",
        "# tsne = TSNE(n_components=2, random_state=42)\n",
        "# X_tsne = tsne.fit_transform(handled_test_data.cpu().numpy())\n",
        "# print(\"handled test data:\", X_tsne)\n",
        "# y_pred = y_pred.cpu().numpy()\n",
        "\n",
        "# print(\"Predicted data\", y_pred)\n",
        "\n",
        "# # Plot the t-SNE visualization\n",
        "# plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y_pred, cmap='RdYlBu')\n",
        "# plt.colorbar()\n",
        "# plt.title('t-SNE Visualization of Weather Features with Rain Labels')\n",
        "# plt.xlabel('t-SNE Dimension 1')\n",
        "# plt.ylabel('t-SNE Dimension 2')\n",
        "# plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}